# **Translation Evaluation with BLEU and chr-F**

This project evaluates the **Helsinki-NLP/opus-mt-tc-big-en-pt** translation model using **BLEU** and **chr-F** metrics. The evaluation is based on the **Tatoeba** dataset, specifically for **English-to-Portuguese** translations.

## **ğŸš€ Features**
- âœ… Uses **Hugging Face Transformers** to load and run the MarianMT translation model.
- âœ… Automatically detects **GPU (CUDA)** for faster inference.
- âœ… Loads **1,000 shuffled sentence pairs** from the **Tatoeba** dataset.
- âœ… Evaluates the model using **BLEU and chr-F** scores.
- âœ… Displays the total number of **sentences and words** processed.

## **ğŸ“Œ How It Works**
1. **Loads the dataset** from Hugging Face and selects 1,000 sentence pairs.
2. **Loads the MarianMT model** and tokenizer for English-to-Portuguese translation.
3. **Generates translations** and compares them with the reference translations.
4. **Computes BLEU and chr-F scores** to measure translation quality.
5. **Prints evaluation results** and dataset statistics.

## **ğŸ›  Installation**
Make sure you have Python and the required dependencies installed:

```bash
pip install torch transformers datasets evaluate tqdm
```

## **â–¶ï¸ Usage**
Run the script:

```bash
python model.py
```

## **ğŸ“Š Example Output**
```
ğŸ”¹ BLEU Score: 38.75  
ğŸ”¹ chr-F Score: 0.56342  
ğŸ”¹ Sentences: 1000  
ğŸ”¹ Words: 14,532  
```

## **ğŸ“œ License**
This project is open-source and available under the MIT License.
